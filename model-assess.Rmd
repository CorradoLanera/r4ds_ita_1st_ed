# Valutazione dei modelli

In questo capitolo, rivolgerete gli strumenti dei modelli multipli verso la valutazione del modello: imparare come il modello si comporta quando gli vengono dati nuovi dati. Finora ci siamo concentrati sui modelli come strumenti di descrizione, usando i modelli per aiutarci a capire i modelli nei dati che abbiamo raccolto finora. Ma idealmente un modello farà più che descrivere ciò che abbiamo visto finora - aiuterà anche a prevedere ciò che verrà dopo.  

In altre parole, vogliamo un modello che non solo si comporti bene sul campione, ma che riassuma accuratamente la popolazione sottostante.

In alcuni settori questo è l'uso principale dei modelli: si spende relativamente poco tempo ad adattare il modello rispetto a quante volte lo si usa.

Ci sono due modi fondamentali in cui un modello può fallire con nuovi dati:

* Si può sotto- o sovra-adattare il modello.  L'underfitting è quando non si riesce a modellare una tendenza importante: si lascia troppo nei residui e non abbastanza nel modello. L'overfitting è l'opposto: si adatta una tendenza a quello che in realtà è un rumore casuale: si è messo troppo modello e non si è lasciato abbastanza nei residui. Generalmente l'overfitting tende ad essere un problema maggiore dell'underfitting.

* Il processo che genera i dati potrebbe cambiare. Non c'è niente che il modello possa fare su questo. Puoi proteggerti da questo in una certa misura creando modelli che capisci e applicando le tue conoscenze al problema. È probabile che questi fondamenti cambino? Se avete un modello che userete ripetutamente per molto tempo, dovete pianificare la manutenzione del modello, controllando regolarmente che abbia ancora senso.
  
    <http://research.google.com/pubs/pub43146.html> <http://www.wired.com/2015/10/can-learn-epic-failure-google-flu-trends/>
 
Il problema più comune con un modello che lo fa funzionare male con i nuovi dati è l'overfitting.


Ovviamente, qui c'è un piccolo problema: non abbiamo nuovi dati con cui controllare il modello, e anche se li avessimo, presumibilmente li useremmo per migliorare il modello in primo luogo. Una potente tecnica di approccio può aiutarci ad aggirare questo problema: il ricampionamento.

Ci sono due principali tecniche di ricampionamento che copriremo.

* Useremo la __convalida incrociata__ (cross validation) per valutare la qualità del modello. Nella convalida incrociata, si dividono i dati in test e training set. Si adattano i dati al set di addestramento e si valutano sul set di test. Questo evita la distorsione intrinseca di usare gli stessi dati sia per adattare il modello che per valutarne la qualità. Tuttavia introduce una nuova distorsione: non state usando tutti i dati per adattare il modello, quindi non sarà così buono come potrebbe essere.
  
* Useremo __boostrapping__ per capire quanto stabile (o quanto variabile) sia il modello. Se si campionano più volte i dati della stessa popolazione, quanto varia il modello? Invece di tornare indietro per raccogliere nuovi dati, potete usare la migliore stima dei dati della popolazione: i dati che avete raccolto finora. L'idea sorprendente del bootstrap è che potete ricampionare dai dati che avete già.

Ci sono molti aiutanti di alto livello per fare questi metodi di ricampionamento in R. Useremo gli strumenti forniti dal pacchetto modelr perché sono espliciti - vedrete esattamente cosa sta succedendo ad ogni passo.

<http://topepo.github.io/caret>. [Applied Predictive Modeling](https://amzn.com/1461468485), di Max Kuhn e Kjell Johnson.

Se si compete in competizioni, come Kaggle, che riguardano principalmente la creazione di buone previsioni, sviluppare una buona strategia per evitare l'overfitting è molto importante. Altrimenti si rischia di ingannare se stessi pensando di avere un buon modello, quando in realtà si ha solo un modello che fa un buon lavoro di adattamento dei dati.

C'è una famiglia strettamente correlata che usa un'idea simile: gli insiemi di modelli. Tuttavia, invece di cercare di trovare i migliori modelli, gli ensemble fanno uso di tutti i modelli, riconoscendo che anche i modelli che non si adattano particolarmente bene a tutti i dati possono ancora modellare bene alcuni sottoinsiemi. In generale, si può pensare alle tecniche di ensemble di modelli come a funzioni che prendono una lista di modelli e restituiscono un singolo modello che cerca di prendere la parte migliore di ognuno.


### Prerequisiti

```{r setup, message = FALSE}
# Manipolazione e visualizzazione dei dati standard
library(dplyr)
library(ggplot2)

# Strumenti per lavorare con i modelli
library(broom)
library(modelr)
library(splines)

# Strumenti per lavorare con molti modelli
library(purrr)
library(tidyr)
```

```{r}
# Opzioni che ti semplificano la vita
options(
  contrasts = c("contr.treatment", "contr.treatment"),
  na.option = na.exclude
)
```


## Overfitting

Sia il bootstrapping che la validazione incrociata ci aiutano a individuare e rimediare al problema dell'__over fitting__, dove il modello si adatta estremamente bene ai dati che abbiamo visto finora, ma fa un cattivo lavoro di generalizzazione a nuovi dati.

Un esempio classico di over-fitting è l'utilizzo di un polinomio con troppi gradi di libertà.

Scambio bias - varianza.  Più semplice = più distorto. Complesso = più variabile.  Rasoio di Occam.

```{r}
true_model <- function(x) {
  1 + 2 * x + rnorm(length(x), sd = 0.25)
}

df <- tibble(
  x = seq(0, 1, length = 20),
  y = true_model(x)
)

df %>% 
  ggplot(aes(x, y)) +
  geom_point()
```

Possiamo creare un modello che si adatta molto bene a questi dati:

```{r, message = FALSE}
library(splines)
my_model <- function(df) {
  lm(y ~ poly(x, 7), data = df)
}

mod <- my_model(df)
rmse(mod, df)

grid <- df %>% 
  expand(x = seq_range(x, 50))
preds <- grid %>% 
  add_predictions(mod, var = "y")

df %>% 
  ggplot(aes(x, y)) +
  geom_line(data = preds) + 
  geom_point()
```

Man mano che si adattano modelli sempre più complicati, l'errore del modello diminuisce:

```{r}
fs <- list(
  y ~ x,
  y ~ poly(x, 2),
  y ~ poly(x, 3),
  y ~ poly(x, 4),
  y ~ poly(x, 5),
  y ~ poly(x, 6),
  y ~ poly(x, 7)
)

models <- tibble(
  n = 1:7, 
  f = fs,
  mod = map(f, lm, data = df),
  rmse = map2_dbl(mod, list(df), rmse)
)

models %>% 
  ggplot(aes(n, rmse)) + 
  geom_line(colour = "grey70") + 
  geom_point(size = 3)
```

Ma pensate che questo modello andrà bene se lo applichiamo a nuovi dati della stessa popolazione?

Nella vita reale non è facile andare a raccogliere i dati. Ci sono due approcci che vi aiutano ad aggirare questo problema. Li introdurrò brevemente qui, e poi andremo più in profondità nelle sezioni seguenti.

```{r}
boot <- bootstrap(df, 100) %>% 
  mutate(
    mod = map(strap, my_model),
    pred = map2(list(grid), mod, add_predictions)
  )

boot %>% 
  unnest(pred) %>% 
  ggplot(aes(x, pred, group = .id)) +
  geom_line(alpha = 1/3)
```

È un po' più facile vedere cosa sta succedendo se zoomiamo sull'asse y:

```{r}
last_plot() + 
  coord_cartesian(ylim = c(0, 5))
```

(Potreste notare che mentre ogni singolo modello varia molto, la media di tutti i modelli sembra non essere così male. Questo dà origine a una tecnica di insieme di modelli chiamata media dei modelli).

Il bootstrapping è uno strumento utile per aiutarci a capire come il modello potrebbe variare se avessimo raccolto un campione diverso dalla popolazione. Una tecnica correlata è la validazione incrociata che ci permette di esplorare la qualità del modello. Funziona dividendo ripetutamente i dati in due parti. Un pezzo, l'insieme di addestramento, è usato per adattarsi, e l'altro pezzo, l'insieme di test, è usato per misurare la qualità del modello.

Il codice seguente genera 100 suddivisioni test-formazione, tenendo ogni volta il 20% dei dati per il test. Poi adattiamo un modello al set di allenamento, e valutiamo l'errore sul set di test:

```{r}
cv <- crossv_mc(df, 100) %>% 
  mutate(
    mod = map(train, my_model),
    rmse = map2_dbl(mod, test, rmse)
  )
cv
```

Ovviamente, un grafico ci aiuterà a vedere la distribuzione più facilmente. Ho aggiunto la nostra stima originale dell'errore del modello come linea verticale bianca (dove lo stesso set di dati è usato sia per l'allenamento che per il test), e potete vedere che è molto ottimista.

```{r}
cv %>% 
  ggplot(aes(rmse)) +
  geom_ref_line(v = rmse(mod, df)) +
  geom_freqpoly(binwidth = 0.2) +
  geom_rug()
```

La distribuzione degli errori è molto asimmetrica: ci sono alcuni casi che hanno errori molto alti. Questi rappresentano campioni in cui ci siamo ritrovati con pochi casi su tutti con valori bassi o alti di x. Diamo un'occhiata:

```{r}
filter(cv, rmse > 1.5) %>% 
  unnest(map(train, as.data.frame)) %>% 
  ggplot(aes(x, .id)) + 
    geom_point() + 
    xlim(0, 1)
```

Tutti i modelli che si adattano particolarmente male sono stati adattati a campioni che hanno mancato la prima o due osservazioni o l'ultima o due. Poiché i polinomi sparano fuori in positivo e in negativo, danno pessime previsioni per quei valori.

Ora che vi abbiamo dato una rapida panoramica e un'intuizione di queste tecniche, entriamo più nel dettaglio.

## Ricampionamenti

### Blocchi di costruzione

Sia il boostrap che la validazione incrociata sono costruiti sopra un oggetto "resample". In Modelr, è possibile accedere a questi strumenti di basso livello direttamente con le funzioni `resample_*`.

Queste funzioni restituiscono un oggetto di classe "resample", che rappresenta il ricampionamento in modo efficiente in termini di memoria. Invece di memorizzare il set di dati ricampionati, memorizza gli indici interi e un "puntatore" al set di dati originale. Questo fa sì che i ricampionamenti occupino molta meno memoria.

```{r}
x <- resample_bootstrap(as_tibble(mtcars))
class(x)

x
```

La maggior parte delle funzioni di modellazione chiama `as.data.frame()` sull'argomento `data`. Questo genera un frame di dati ricampionato. Poiché viene chiamata automaticamente, potete semplicemente passare l'oggetto.

```{r}
lm(mpg ~ wt, data = x)
```

Se ottenete uno strano errore, probabilmente è perché la funzione di modellazione non lo fa, e dovete farlo voi stessi. Dovrete farlo voi stessi anche se volete `unnest()` i dati in modo da poterli visualizzare.  Se vuoi solo ottenere le righe selezionate, puoi usare `as.integer()`.

### Dataframe API

`bootstrap()` e `crossv_mc()` sono costruiti sulla base di queste primitive più semplici. Sono progettate per lavorare naturalmente in un ambiente di esplorazione del modello, restituendo dei dataframe. Ogni riga del data frame rappresenta un singolo campione. Essi restituiscono colonne leggermente diverse:

* `boostrap()` restituisce un data frame con due colonne:

    ```{r}
    bootstrap(df, 3)
    ```
    
    `strap` dà il set di dati del campione di bootstrap, e `.id` assegna un identificatore univoco a ciascun modello (questo è spesso utile per i grafici)
    
* `crossv_mc()` restituisce un frame di dati con tre colonne:

    ```{r}
    crossv_mc(df, 3)
    ```
    `train` contiene i dati che dovresti usare per adattare (addestrare) il modello, e `test` contiene i dati che dovresti usare per validare il modello. Insieme, le colonne test e train formano una partizione esclusiva dell'intero set di dati.

## Riassunti numerici della qualità del modello

Quando si inizia ad avere a che fare con molti modelli, è utile avere un modo approssimativo per confrontarli in modo da poter spendere il proprio tempo guardando i modelli che fanno il miglior lavoro nel catturare le caratteristiche importanti nei dati.

Un modo per catturare la qualità del modello è riassumere la distribuzione dei residui. Per esempio, potreste guardare i quantili dei residui assoluti. Per questo set di dati, il 25% delle previsioni sono a meno di \7.400 dollari, e il 75% sono a meno di \25.800 dollari. Sembra un bel po' di errore quando si predice il reddito di qualcuno!

```{r}
heights <- tibble(readRDS("data/heights.RDS"))
h <- lm(income ~ height, data = heights)
h 

qae(h, heights)
range(heights$income)
```

Potreste avere familiarità con il $R^2$. Si tratta di un singolo numero riassuntivo che scala la varianza dei residui tra 0 (molto male) e 1 (molto bene):

```{r}
rsquare(h, heights)
```

$R^2$ può essere interpretato come la quantità di variazione nei dati spiegata dal modello. Qui stiamo spiegando il 3% della variazione totale - non molto! Ma non credo che preoccuparsi della quantità relativa di variazione spiegata sia così utile; penso invece che tu debba considerare se la quantità assoluta di variazione spiegata sia utile per il tuo progetto.

Si chiama $R^2$ perché per modelli semplici come questo, è solo il quadrato della correlazione tra le variabili:

```{r}
cor(heights$income, heights$height) ^ 2
```

Il $R^2$ è un buon riassunto di un singolo numero, ma preferisco pensare ai residui non scalati perché è più facile da interpretare nel contesto dei dati originali. Come imparerai più tardi, è anche un'interpretazione piuttosto ottimistica del modello. Poiché state valutando il modello usando gli stessi dati che sono stati usati per adattarlo, dà davvero più di un limite superiore sulla qualità del modello, non una valutazione equa.



## Bootstrapping


## Validazione incrociata

